{
  "metainfo" : {
    "id" : "1",
    "language" : "scala",
    "fabricId" : "2730",
    "frontEndLanguage" : "sql",
    "mode" : "batch",
    "udfs" : {
      "language" : "scala",
      "udfs" : [ ],
      "functionPackageName" : "abhishekse2etestsprophecy.io_team.scalaproject.functions",
      "sharedFunctionPackageNames" : [ ]
    },
    "udafs" : {
      "language" : "scala",
      "code" : "package udfs\n\nimport org.apache.spark.sql.expressions._\nimport org.apache.spark.sql.types._\nimport org.apache.spark.sql._\n\n/**\n  * Here you can define your custom aggregate functions.\n  *\n  * Make sure to register your `udafs` in the register_udafs function below.\n  *\n  * Example:\n  *\n  * object GeometricMean extends UserDefinedAggregateFunction {\n  *   // This is the input fields for your aggregate function.\n  *   override def inputSchema: org.apache.spark.sql.types.StructType =\n  *     StructType(StructField(\"value\", DoubleType) :: Nil)\n  *\n  *   // This is the internal fields you keep for computing your aggregate.\n  *   override def bufferSchema: StructType = StructType(\n  *     StructField(\"count\", LongType) ::\n  *     StructField(\"product\", DoubleType) :: Nil\n  *   )\n  *\n  *   // This is the output type of your aggregatation function.\n  *   override def dataType: DataType = DoubleType\n  *\n  *   override def deterministic: Boolean = true\n  *\n  *   // This is the initial value for your buffer schema.\n  *   override def initialize(buffer: MutableAggregationBuffer): Unit = {\n  *     buffer(0) = 0L\n  *     buffer(1) = 1.0\n  *   }\n  *\n  *   // This is how to update your buffer schema given an input.\n  *   override def update(buffer: MutableAggregationBuffer, input: Row): Unit = {\n  *     buffer(0) = buffer.getAs[Long](0) + 1\n  *     buffer(1) = buffer.getAs[Double](1) * input.getAs[Double](0)\n  *   }\n  *\n  *   // This is how to merge two objects with the bufferSchema type.\n  *   override def merge(buffer1: MutableAggregationBuffer, buffer2: Row): Unit = {\n  *     buffer1(0) = buffer1.getAs[Long](0) + buffer2.getAs[Long](0)\n  *     buffer1(1) = buffer1.getAs[Double](1) * buffer2.getAs[Double](1)\n  *   }\n  *\n  *   // This is where you output the final value, given the final value of your bufferSchema.\n  *   override def evaluate(buffer: Row): Any = {\n  *     math.pow(buffer.getDouble(1), 1.toDouble / buffer.getLong(0))\n  *   }\n  * }\n  *\n  */\n\n\nobject UDAFs {\n  /**\n    * Registers UDAFs with Spark SQL\n    */\n  def registerUDAFs(spark: SparkSession): Unit = {\n    /**\n      * Example:\n      *\n      * spark.udf.register(\"gm\", GeometricMean)\n      *\n      */\n\n\n  }\n}\n"
    },
    "configuration" : {
      "common" : {
        "type" : "record",
        "fields" : [ ]
      },
      "oldCommon" : {
        "type" : "record",
        "fields" : [ ]
      },
      "fabrics" : { },
      "instances" : { },
      "selected" : "default",
      "nonEditable" : [ ],
      "isSubscribedPipelineWithPipelineConfigs" : false
    },
    "sparkConf" : [ ],
    "hadoopConf" : [ ],
    "codeMode" : "sparse",
    "buildSystem" : "maven",
    "externalDependencies" : [ {
      "type" : "coordinates",
      "coordinates" : "mysql:mysql-connector-java:8.0.29",
      "name" : "mysql",
      "enabled" : false,
      "id" : "59965868",
      "exclusions" : [ ]
    }, {
      "type" : "coordinates",
      "coordinates" : "org.postgresql:postgresql:42.3.4",
      "name" : "postgres",
      "enabled" : false,
      "id" : "1360721423",
      "exclusions" : [ ]
    }, {
      "type" : "coordinates",
      "coordinates" : "org.scalanlp:epic_2.12:0.5",
      "name" : "epic",
      "enabled" : false,
      "id" : "418005485",
      "exclusions" : [ ],
      "repo" : "https://repo.maven.apache.org/maven2/"
    }, {
      "type" : "coordinates",
      "coordinates" : "org.typelevel:cats-core_2.12:2.6.1",
      "name" : "cats",
      "enabled" : false,
      "id" : "1575366423",
      "exclusions" : [ ]
    }, {
      "type" : "coordinates",
      "coordinates" : "org.springframework:spring-beans:5.3.19",
      "name" : "spring",
      "enabled" : false,
      "id" : "611418782",
      "exclusions" : [ ]
    }, {
      "type" : "coordinates",
      "coordinates" : "com.crealytics:spark-excel_2.12:3.2.1_0.17.1",
      "name" : "spark-excel",
      "enabled" : false,
      "id" : "368609749",
      "exclusions" : [ ]
    }, {
      "type" : "coordinates",
      "coordinates" : "io.prophecy:prophecy-libs_2.12:4.0.0-3.2.0",
      "name" : "prophecy-libs-4.0.0",
      "enabled" : false,
      "id" : "GXM_V",
      "exclusions" : [ ],
      "repo" : "https://prophecyio.jfrog.io/artifactory/default-sbt-release/"
    }, {
      "type" : "coordinates",
      "coordinates" : "junit:junit:4.13.1",
      "name" : "junit",
      "enabled" : false,
      "id" : "O0zCh",
      "exclusions" : [ "" ]
    } ],
    "dependentProjectExternalDependencies" : [ ],
    "isImported" : false,
    "interimMode" : "Full",
    "interimModeEnabled" : true,
    "visualCodeInterimMode" : "Disabled",
    "recordsLimit" : {
      "enabled" : false,
      "value" : 1000
    },
    "topLevelPackage" : "io.prophecy.pipelines.scala_vector_type",
    "configurationVersion" : "v1"
  },
  "connections" : [ {
    "id" : "iygOPIZnT_tCRsgbO1wgZ",
    "source" : "mnaEOKQ4lv_aJFvi4TAhU$$BNrplYfy_pQxDQlDmIgTS",
    "sourcePort" : "O0b_eH8LWJU9XafrndxTu$$xgNqUhv7h7CKlz-qAqmHU",
    "target" : "iS0KQ5Q0Ar5Effk9_erie$$7MXljkXtWl83MMSKxG64F",
    "targetPort" : "ucEYm1mGx8xjr0VQL63NT$$mtmB4NJZxL9D_5exNL_CG"
  }, {
    "id" : "9BMIy9MSuHsWps_w-Qx8o",
    "source" : "iS0KQ5Q0Ar5Effk9_erie$$7MXljkXtWl83MMSKxG64F",
    "sourcePort" : "J0WA5Ul6Oz52OkRjkgK5R$$VGXNPc0yrQFrjcj_KllTP",
    "target" : "-lPxVDjFwv4B4036w--20$$HbaMOhlPg0GXJqgmMuRAp",
    "targetPort" : "qbZiwfZvzUsqQlAvYqlMe$$HzTuacagMEcHq2msyE3ds"
  } ],
  "processes" : {
    "mnaEOKQ4lv_aJFvi4TAhU$$BNrplYfy_pQxDQlDmIgTS" : {
      "id" : "mnaEOKQ4lv_aJFvi4TAhU$$BNrplYfy_pQxDQlDmIgTS",
      "component" : "Source",
      "metadata" : {
        "label" : "normalization_csv_dataset",
        "slug" : "normalization_csv_dataset",
        "x" : 140,
        "y" : 160,
        "phase" : 0,
        "cache" : false,
        "detailedStats" : false,
        "isImported" : false
      },
      "ports" : {
        "inputs" : [ ],
        "outputs" : [ {
          "id" : "O0b_eH8LWJU9XafrndxTu$$xgNqUhv7h7CKlz-qAqmHU",
          "slug" : "out"
        } ],
        "selectedInputFields" : [ ],
        "isCustomOutputSchema" : false
      },
      "properties" : {
        "datasetId" : "datasets/normalization_csv_dataset"
      }
    },
    "iS0KQ5Q0Ar5Effk9_erie$$7MXljkXtWl83MMSKxG64F" : {
      "id" : "iS0KQ5Q0Ar5Effk9_erie$$7MXljkXtWl83MMSKxG64F",
      "component" : "Script",
      "metadata" : {
        "label" : "Script_1",
        "slug" : "Script_1",
        "x" : 360,
        "y" : 160,
        "phase" : 0,
        "cache" : false,
        "detailedStats" : false,
        "isImported" : false
      },
      "ports" : {
        "inputs" : [ {
          "id" : "ucEYm1mGx8xjr0VQL63NT$$mtmB4NJZxL9D_5exNL_CG",
          "slug" : "in0",
          "schema" : {
            "type" : "struct",
            "fields" : [ {
              "name" : "Y_Value_of_Occupied_Homes",
              "type" : "double",
              "nullable" : true,
              "metadata" : {
                "description" : "",
                "mappings" : [ ],
                "tags" : [ ]
              }
            }, {
              "name" : "Crime_Rate",
              "type" : "double",
              "nullable" : true,
              "metadata" : {
                "description" : "",
                "mappings" : [ ],
                "tags" : [ ]
              }
            }, {
              "name" : "Residential_Land_Zone",
              "type" : "double",
              "nullable" : true,
              "metadata" : {
                "description" : "",
                "mappings" : [ ],
                "tags" : [ ]
              }
            }, {
              "name" : "Non_retail_Business_acres",
              "type" : "double",
              "nullable" : true,
              "metadata" : {
                "description" : "",
                "mappings" : [ ],
                "tags" : [ ]
              }
            }, {
              "name" : "Charles_River",
              "type" : "double",
              "nullable" : true,
              "metadata" : {
                "description" : "",
                "mappings" : [ ],
                "tags" : [ ]
              }
            }, {
              "name" : "Nitric_Oxide",
              "type" : "double",
              "nullable" : true,
              "metadata" : {
                "description" : "",
                "mappings" : [ ],
                "tags" : [ ]
              }
            }, {
              "name" : "Average_Rooms",
              "type" : "double",
              "nullable" : true,
              "metadata" : {
                "description" : "",
                "mappings" : [ ],
                "tags" : [ ]
              }
            }, {
              "name" : "Owner_Occupied_Units",
              "type" : "double",
              "nullable" : true,
              "metadata" : {
                "description" : "",
                "mappings" : [ ],
                "tags" : [ ]
              }
            }, {
              "name" : "Distance_to_Employment_Centers",
              "type" : "double",
              "nullable" : true,
              "metadata" : {
                "description" : "",
                "mappings" : [ ],
                "tags" : [ ]
              }
            }, {
              "name" : "Accessibility_to_Highways",
              "type" : "double",
              "nullable" : true,
              "metadata" : {
                "description" : "",
                "mappings" : [ ],
                "tags" : [ ]
              }
            }, {
              "name" : "Property_Tax_Rate",
              "type" : "double",
              "nullable" : true,
              "metadata" : {
                "description" : "",
                "mappings" : [ ],
                "tags" : [ ]
              }
            }, {
              "name" : "Pupil_Teacher_Ratio",
              "type" : "double",
              "nullable" : true,
              "metadata" : {
                "description" : "",
                "mappings" : [ ],
                "tags" : [ ]
              }
            }, {
              "name" : "Lower_Status",
              "type" : "double",
              "nullable" : true,
              "metadata" : {
                "description" : "",
                "mappings" : [ ],
                "tags" : [ ]
              }
            } ]
          },
          "isStreaming" : false
        } ],
        "outputs" : [ {
          "id" : "J0WA5Ul6Oz52OkRjkgK5R$$VGXNPc0yrQFrjcj_KllTP",
          "slug" : "out0",
          "schema" : {
            "type" : "struct",
            "fields" : [ ]
          },
          "isStreaming" : false
        } ],
        "selectedInputFields" : [ ],
        "isCustomOutputSchema" : true
      },
      "properties" : {
        "script" : "// Import Spark.ml libraries\nimport org.apache.spark.ml.regression.LinearRegression\nimport org.apache.spark.ml.feature.VectorAssembler\n\n// Prepare the data by assembling features into a single vector column\n\nval assembler = new VectorAssembler()\n  .setInputCols(Array(\"Crime_Rate\", \"Residential_Land_Zone\", \"Non_retail_Business_acres\", \"Charles_River\", \"Nitric_Oxide\", \"Average_Rooms\", \"Owner_Occupied_Units\", \"Distance_to_Employment_Centers\", \"Accessibility_to_Highways\", \"Property_Tax_Rate\", \"Pupil_Teacher_Ratio\", \"Lower_Status\"))\n  .setOutputCol(\"features\")\n\nval df = assembler.transform(in0)\n\n// Split data into training and test\nval Array(train, test) = df.randomSplit(Array(0.7, 0.3))\n\n// create linear regression model\nval lr = new LinearRegression()\n  .setLabelCol(\"Y_Value_of_Occupied_Homes\")\n  .setFeaturesCol(\"features\")\n\n  // fit the model to the training data\nval model = lr.fit(train)\n\n// Make predictions on the test data\nval predictions = model.transform(test)\n\n//Dropping \"features\" vector column will result in dataframe output working\n// val predictions2 = predictions.drop(\"features\")\n\nval out0 = predictions",
        "scriptMethodHeader" : "def apply(spark: SparkSession, in0: DataFrame): DataFrame = {",
        "scriptMethodFooter" : "    out0\n}"
      }
    },
    "-lPxVDjFwv4B4036w--20$$HbaMOhlPg0GXJqgmMuRAp" : {
      "id" : "-lPxVDjFwv4B4036w--20$$HbaMOhlPg0GXJqgmMuRAp",
      "component" : "Reformat",
      "metadata" : {
        "label" : "Reformat_1",
        "slug" : "Reformat_1",
        "x" : 640,
        "y" : 220,
        "phase" : 0,
        "cache" : false,
        "detailedStats" : false,
        "isImported" : false
      },
      "ports" : {
        "inputs" : [ {
          "id" : "qbZiwfZvzUsqQlAvYqlMe$$HzTuacagMEcHq2msyE3ds",
          "slug" : "in",
          "schema" : {
            "type" : "struct",
            "fields" : [ ]
          },
          "isStreaming" : false
        } ],
        "outputs" : [ {
          "id" : "NU_G2qbLlv8Y4w6oadPM1$$MxRAD5e2n66POVM_zmTbn",
          "slug" : "out"
        } ],
        "selectedInputFields" : [ ],
        "isCustomOutputSchema" : false
      },
      "properties" : {
        "columnsSelector" : [ ],
        "expressions" : [ ]
      }
    }
  },
  "ports" : {
    "inputs" : [ ],
    "outputs" : [ ],
    "selectedInputFields" : [ ],
    "isCustomOutputSchema" : false
  }
}